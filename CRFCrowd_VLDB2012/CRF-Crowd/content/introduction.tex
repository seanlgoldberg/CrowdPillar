\section{Introduction}
As the ability to acquire, maintain, and store large amounts of data becomes increasingly easier and more affordable, so does the ability to employ a wide variety of data analyses.  A growing trend is in the study of probabilistic data analyses through the application of statistical and machine learning models.  Sample applications include businesses modeling their customers' activity through clickstreams and providing recommendations, or the information extraction and entity resolution from raw unstructured text.

One approach for processing, managing, and storing these massive amounts of probabilistic data is to utilize a Probabilistic Database (PDB) \cite{Dalvi07}.  While traditional relational databases have difficulty storing the inherent uncertainty that results from machine learning techniques, PDBs are able to model and provide declarative queries over uncertain data.

The first Probabilistic Databases that were built relied on simple models of uncertainty that could be easily mapped onto existing relational architectures \cite{Barbara92}.  This introduced a mismatch between the models used to process the data and those used to store the data.  A number of more recent systems have attempted to solve this "model-mismatch" problem by establishing the Statistical Machine Learning (SML) models and inference algorithms as first class citizens in the PDB.  Common Probabilistic Graphical Models (PGMs) such as Bayesian Networks and Conditional Random Fields have already been implemented effectively within the database \cite{Sen09, Wang08, Wang11}.  

SML is not perfect and for some tasks SML can achieve an accuracy as high as 97\% while for others (such as image recognition) it is only able to achieve 60\%.  There are cases where the model is unable to adequately reason about a difficult piece of data and as a result introduces large uncertainties into the data.  The need for a new type of data cleaning process has emerged.  Previous uses of data cleaning have been to reconcile incorrect or corrupt pieces of data in a deterministic database.  For a PDB, a necessary procedure is to reconcile those data which are most uncertain by providing human input.  

In addition to utilize machine learning techniques for large scale analysis, an increasing trend has been to harness human computation in a distributed manner using crowdsourcing.  Benefits can be found in problems that are too difficult or expensive for computers.  Services like Amazon Mechanical Turk (AMT) have led the way by setting up an infrastructure that allows payment for the combined resources of up to hundreds of thousands of people.

CrowdPillar is a system designed to harness the power of crowdsourcing applications like AMT to reduce the uncertainty and improve the reliability of PDB Systems with an inherent graphical model structure.  Certain SML applications like information extraction can be performed easily by most humans, but require machine processing to scale with large amounts of data.  Compared to traditional single user data cleaning (like that used in active learning), CrowdPillar employs the relative speed and accesibility of the crowd for large scale probablistic data cleaning.  Given a graphical model operating on large scale data, CrowdPillar uses methods associated with entropy to pinpoint the weakest and most uncertain nodes in the graph and automatically queries the crowd for correction.  Since the crowd may not agree on the results, we combine the response using the Dempster-Shafer theory of evidence before integration into the existing PDB.

We make three main contributions with this paper.  First, we present a new algorithm for formulating a list of questions for the crowd ordered by the importance of their answers.  These questions are derived from PDBs with a graphical model as their underlying structure.  The second contribution is in the application of Dempster-Shafer's Theory of Evidence for combining data from multiple sources, including both the crowd and the SML model.  Finally, we present a series of experiments involving real and synthetic data to demonstrate the effectiveness of the previous two contributions.

This paper is organized as follows.  Section 2 provides a background on PDBs in general as well as some of the theoretical topics necessary to understand how CrowdPillar works.  In Section 3, we give an overview of the CrowdPillar system and discuss the process of data flow through the system.  The uncertainty in PDBs needs to be reformulated into questions posed to the crowd.  Different methods for assigning which questions should be asked are discussed in Section 4.  Section 5 overviews Dempster-Shafer theory as an alternative to the common majority voting procedure.  Our experimental setup and results can be found in Section 6 while Section 7 contains conclusions drawn and showcases future work to be done.
