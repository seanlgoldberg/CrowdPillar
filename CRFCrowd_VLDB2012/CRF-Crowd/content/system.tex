\section{System Overview}
The CrowdPillar system design is shown in (FIGURE). The core of the system is designed as a modification to probabilistic databases that utilize PGMs as their data model.  Crowdpillar analyzes the uncertainty in the output and generates questions for crowd submission to reduce some Total Utility Function (TUF).  This is discussed in greater detail in (SECTION).  The response of the crowd is combined in a principled way with the original PGM output using Dempter-Shafer belief theory, as described in (SECTION).

We consider a sample application task and follow it through the major components of the system: the probabilistic database, question formulation, crowd submission, and data fusion.

Consider an extraction system for automatically reading published citations and storing them in the database.  In order to fit an incoming citation into any nontrivial schema, the string has to be appropriately chunked to determine which tokens belong to which attributes.  A string such as:
\begin{quotation}
\textit{Perfect Model Checking via Unfold/Fold Transformations. Maurizio Proietti, Alberto Pettorossi CL 3-540-67797-6 Springer Lecture Notes in Computer Science Computational Logic - CL 2000, First International Conference, London, UK, 24-28 July, 2000, Proceedings 2000}
\end{quotation}
needs to be labeled with its appropriate title, author, conference, etc.  This is in general a very difficult problem, especially when one considers the numerous styles and orderings different citations come in.  An efficient extraction system must be trained to handle these different occurences.

Despite the difficulty, conditional random fields are particularly well suited to this information extraction task.  It is for this reason that we focus on CRFs for the remainder of the paper and make it the flagship example of CrowdPillar, though many other IE methods that can derive the appropriately labelings may be used.  The parameters associated with the CRF allow us to designate in a simple way which questions should be provided to the crowd. 

The goal of CrowdPillar is to optimally select a number of sequences of high uncertainty from the CRF output and query the crowd for the label of each token in a sequence with the highest individual score.  We discuss specifically how scoring is done in Section 4.  How many questions can be asked depends on the user's cost and time budget.  An on-line approach to selecting sequences is highly inefficient due to the relatively slow speed of achieving a crowd response (on the order of hours) as well as the high parallelizability of largely independent sequences.  For these reasons, sequences are selected in a batch for submission to the crowd.

After determining which sequences should be sent to the crowd, we generate a question in XML format for each sequence.  Questions may be in the form of multiple choice, fill in the label, yes/no, etc.  Given the possible uncertainty in the crowd response, we resolve conflict and combine responses using Dempster-Shafer theory based on the Turkers' approval ratings to generate a probabilistic collection of responses to each question.

The crowd submissions are sent back to the database, where they are combined with the original output CRF data using the Dempster-Shafer Theory of Evidence.  The answers can be returned at once upon completion of the batch or queried in intervals.  How often to specifically query for answers will be dependent on the user's specifications and needs. 

